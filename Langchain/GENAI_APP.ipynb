{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6abf7fb7",
   "metadata": {},
   "source": [
    "# Chat with Website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c98c6",
   "metadata": {},
   "source": [
    "### This is a gemini based app that uses RAG to enable users chat with any website content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae70d41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c13da33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"LANGHCAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853824ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Paris', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-1.5-flash', 'safety_ratings': []}, id='run--0da72978-d89e-4ada-ba92-810103bb7372-0', usage_metadata={'input_tokens': 7, 'output_tokens': 2, 'total_tokens': 9, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.1,\n",
    "    max_output_tokens=1024\n",
    ")\n",
    "\n",
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074f1fbb",
   "metadata": {},
   "source": [
    "### Step1: Data Loading from website\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb179af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://towardsdatascience.com/software-engineering-in-the-llm-era/', 'title': 'Software Engineering in the LLM Era | Towards Data Science', 'language': 'en-US'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSoftware Engineering in the LLM Era | Towards Data Science\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPublish AI, ML & data-science insights to a global community of data professionals.\\n\\n\\nSign in\\nSign out\\nSubmit an Article\\n\\n\\n\\n\\n\\n\\nLatestEditor’s PicksDeep DivesNewsletter\\n\\nWrite For TDS\\n\\n\\n \\n\\n\\n\\n \\n\\n \\nToggle Mobile Navigation\\n\\n\\n\\n\\nLinkedIn\\nX\\n\\n\\n\\n \\n\\n \\nToggle Search\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\t\\tLarge Language Models\\t\\n\\nSoftware Engineering in the LLM Era\\n\\n\\n\\n\\t\\t\\tOn growing new software engineers, even when it’s inefficient\\t\\t\\n\\n\\n\\n\\nStephanie Kirmer\\nJul 2, 2025\\n\\n\\t13 min read\\n\\n\\n\\n\\n\\n\\t\\tShare\\t\\n\\n\\nPhoto by Francesco Gallarotti on Unsplash\\n\\nIn the broad software engineering space, debates about whether using LLMs in coding is a good or a bad idea are raging. On the extremes, there are some people who think that any use of LLMs in coding is indefensible, and other people are ready to throw out the whole field of software engineering as too expensive and unnecessary. I’m fascinated by this discussion, particularly from my sociologist’s perspective, because so much of the conversation seems to be about whether an LLM is useful. Is it so fatally flawed that it’s a total waste of time, or is it so unimaginably powerful that it’s a fool’s errand to avoid it? Instead of thinking about it so narrowly, I actually really want to talk about the broader context of software engineering in the context of LLM technology. (I consider software engineering only adjacent to my field of machine learning engineering, but I have worked alongside many extremely talented SWEs in my career, at tech companies of various sizes, and I’ve had the chance to observe the role quite closely. Some of the analysis below also applies to the MLE field, but not all of it.)\\nContext\\nDuring my own career I’ve seen a lot change in the American tech economy, and in the way the job of “coding” has been perceived and described. It’s a bit of a trope but it was certainly true for a period of time that “learning to code” was touted as the fastest way to a white collar career with a steady paycheck and future-proof prospects, both for young people and for career-changers trying to find a place in the shifting economy. Coding bootcamps came and (mostly) went, for a time giving people a relatively quick way to gain the foundational skills of the job.\\nIn many fields, however, shortages of qualified employees can swiftly turn into a glut of talent with nowhere to go (ask lawyers), especially when high or at least comfortable salaries are on offer. While some people think this is happening in software engineering, it’s not clear if that’s necessarily the case yet, because there are other factors in play too. In the mid-2010s, many tech companies and startups were scrambling for hyper-growth and hiring as fast as possible, using cheap venture capital to fund their expansions without regard for profitability, but that’s definitively over now. Tech layoffs, including many software engineers, exploded in the years immediately following the start of the COVID pandemic, in large part as a response to the unsustainable hiring of preceding years and the sudden jolt to the economy of lockdowns. After that period of layoffs, hiring for SWE talent slowed in many sectors. Developer skillsets are diversifying and specializing, so there may still be sub-fields with strong hiring, but a lot of young SWEs in particular are struggling to break in.\\nJunior vs\\xa0Senior\\nThis brings us around to now. Some people argue that there aren’t too many software engineers, but there are too many inexperienced or junior software engineers, and that employers are still desperate for experienced hires. What does that really mean? Is it true? Without doing academic research to really investigate (if anyone has done good research on this question, please send it to me!), we can still take a look at the issues involved. First, let’s define terms.\\n\\nI’m not using anybody’s in-house leveling terms, or talking about job titles here, but I want to clarify what the differences might be in someone’s role depending on the experience and skills they bring.\\n\\nA junior SWE is someone who needs quite a bit of support, coaching, and managing to produce good quality work. They’re not yet at a place of working independently or mentoring others. They are still learning how the job is done. This doesn’t mean they don’t produce value! Junior engineers do write code and get work done, and they can often learn very quickly and become more productive. However, a senior SWE is someone who can be expected to work mostly independently, with minimal supervision required for good work to be done and goals to be met, and they can be relied on to help those junior folks along. They start to be able to solve more difficult problems, write elegant code that is high quality, and even begin being part of strategy conversations about how to build your whole codebase for the long haul.\\nThis is more or less the pattern for most skilled professions\\u200a—\\u200ayou’ll learn some amount of what you need to know from schooling or training, but you really learn how to be a member of the profession and do the job from experience and practice. There’s no set rule about how much time it takes for you to go from junior to senior in this framework, because it’s all about what you can do and the capabilities you have. Your perceived value as a worker (and your pay) go up as you get better at the job over time.\\nLLMs Enter Stage\\xa0Left\\nWith this, we can start to inquire about how that shifts, and how the existence of LLMs and LLM based coding tools actually may (to use a cliche) disrupt the context of the profession now.\\nAs an individual, if you’re a senior SWE, you might find an LLM tool like Github Copilot or Cursor handy for doing scut work, such as getting unit tests working, writing basic docs, or sketching out the start of a project that you’ll then fill in based on your own knowledge. However, your own knowledge is really vital for this to be effective. As we in machine learning will tell you, LLM output is probabilistic at its core. If the training data has many examples of code that are relevant to what you’re doing, it’s going to be reasonably likely to produce code for you that works. But sooner or later (probably sooner) it’s going to generate something that won’t work. That might be failing loudly or failing silently, it might be opening up a security hole, or it might just be a suboptimal way of doing a task. The thing that matters at that point is what you do in response.\\nAssuming you are a senior SWE with significant experience writing all this stuff by hand yourself, you have the skills to spot the mistakes, and figure out how to fix them smoothly so the end product works well. The LLM probably makes you more productive in the end, because you could abstract away some of the work that isn’t interesting and doesn’t require much thought. If you’re not a senior person, well, then you may end up going back to the LLM to ask it to fix its own mistakes. And it might do that, but there’s always a risk of another mistake being introduced, and on down the road. There’s really no telling how long this could take, and it’s hard to compare the time with how long it would have taken you to learn the underlying code and just solve it yourself.\\nHowever, lots of people will argue that the LLM repeating workflow is fine. People may believe that just being there to click the hover button to tell Copilot to “Fix” over and over again is sufficient, even if it’s not a particularly satisfying job. However, there are serious arguments to be made that the output will be seriously flawed, whether in readability, maintainability, security holes, or other aspects.\\n\\n“Sure, Gen AI supercharges development, but it also supercharges risk. Two engineers can now churn out the same amount of insecure, unmaintainable code as 50 engineers.”\\u200a—\\u200ahttps://www.zdnet.com/article/10-professional-developers-on-vibe-codings-true-promise-and-peril/\\n\\nThis question has been addressed by many writers recently. However, I’m less interested at this moment in whether Copilot can get to the desired result eventually than I am in what this process and way of working does to us.\\nBroaden the\\xa0Aperture\\nRemember that we’re here to talk about the effect of the LLM on the profession. In the pre-AI era, a junior engineer would start working on a ticket, and make some mistakes, introduce a bug or two, and they’d basically be armed with StackOverflow and the docs, plus peers to collaborate with for help, to figure out how to get it right. Once they muddled through and got the code working, they’d make a PR and get feedback from more experienced peers, who would offer recommendations and corrections. Once those corrections were made, a PR could be approved and merged.\\nI think the “muddling through” part of this process is actually pedagogically and professionally quite important. Once you’ve passed through the academic preparation for the software engineering role, the practical preparation is what you need to develop the capabilities to grow. I am a strong believer in the idea that making mistakes and fixing them is a tremendously valuable way to learn. Once the junior dev working on that bit of code has worked hard, fixed their own bugs, written the tests and docs, and gone through PR review, they know that bit of code better than any other person around. That process is the productive cognitive labor that brings them a step closer to being an engineer who can work independently, and take on more senior responsibilities successfully.\\nOn Being Efficient\\nHowever, from the outside, this process looks slow. It looks like they had to spend a lot of time flailing around to just get to the end result of code that works, and surely it would be more efficient to either just make an LLM do it, or make a senior dev do it?\\nIf efficiency is your only criteria, then sure, maybe this is true. But I want to push back strongly on the premise. We do lots of things without thinking about broader context or impact, especially in the tech industry, and prioritizing efficiency today at all costs is a recipe for future disaster. Let’s spin out the narrative a bit\\u200a—\\u200aif efficiency is the priority, either we have an LLM do the bulk of the work, or a senior dev does it, or maybe a bit of both, like an LLM does the first stab and a senior dev reviews it. Setting aside any questions of quality (about which there should be questions), what’s the role of a junior SWE in this space?\\nDoes a junior SWE watch the LLM do most of the coding? Do they just exist as an observer? Do they click “ok” from time to time as the LLM goes along, exemplifying “vibe coding”? Or do they not exist at all in this scenario? Loud voices in the AI space certainly are arguing that they disappear altogether. If so, in the immediate moment, employers in tech save a significant amount of money on salaries, so stock price goes up.\\n\\nThe idea of vibe coding is using an LLM based tool to write the code for your project, with minimal actual hand-writing of code involved. The name comes from the principle that you behind the keyboard have a general sense of the “vibe” of what you want your result to be, but you either don’t know how to do it yourself or don’t want to do it yourself, so you’re outsourcing the cognitive work of coding to a computer. You’ll just describe the broad strokes of what you want, in human language, and let the LLM figure out a way to achieve that through code.\\n\\nHollowing\\nHowever, we know that a healthy profession needs to be bringing in new talent as senior staff progress and eventually retire. Senior engineers are still being sought after, because they can solve the problems and write the complex code that neither junior devs nor LLMs can produce. Their experience and skills are undeniably valuable and necessary to good software being produced. But where do we think senior engineers come from? Just like senior doctors, or senior attorneys, they start out as junior people. Their capabilities must be acquired through experience.\\nIf we build a working environment where junior software engineers no longer exist, we are hollowing out the pipeline of talent the profession needs. Instead of having a funnel of talent from academic study into the field, young people have no real way to get to the level of experience that is in highest demand.\\nConclusions\\nI appreciate readers coming along with me on this discussion, because it’s important to think about the technology we make ubiquitous not just from our own individual perspectives but from a broader view. How we employ LLM technology in the field of software engineering will have a serious impact on the future of the profession and what it looks like\\u200a—\\u200aand, I’d argue, this is true for many other fields as well where incorporating LLMs is becoming fashionable. I’m not arguing for zero LLM usage in coding, by any means\\u200a—\\u200ait can be incredibly useful if you have the experience and skills to make the most of it. (There are many important conversations to have about how it may impact our skill levels and the maintenance of our skills, but that’s a different topic for another day.)\\nBut in any profession, senior practitioners don’t just appear\\u200a—\\u200athey’re created, and that process of creation takes work and time, and may not be the most immediately “efficient” thing to do. Efficiency in the immediate moment just isn’t the most important thing\\u200a—\\u200ait can’t be, if we want the future to be bright.\\n\\nI am speaking at two events in July 2025, including a free virtual conference called “Agents in Production” — please sign up if you’re interested!\\nRead more of my work at www.stephaniekirmer.com.\\n\\nFurther Reading\\nAI Killed My Job: Tech workersTech workers at TikTok, Google, and across the industry share stories about how AI is changing, ruining, or replacing…www.bloodinthemachine.com\\n10 professional developers on vibe coding’s true promise and perilIs vibe coding the future of software or a security nightmare in disguise? Here’s what experienced developers think…www.zdnet.com\\nFrontiers | Some Evidence on the Cognitive Benefits of Learning to CodePlease see the submitted files.www.frontiersin.org\\nTrusting your own judgement on ‘AI’ is a huge riskWeb dev at the end of the world, from Hveragerði, Icelandwww.baldurbjarnason.com\\nJones, Capers. 2014. The Technical and Social History of Software Engineering.\\nLayoffs.fyi\\u200a—\\u200aTech Layoff Tracker and DOGE Layoff TrackerLIVE] Tracking all tech startup layoffs\\u200a—\\u200aand lists of employees laid off\\u200a—\\u200asince COVID-19 was declared a pandemic…layoffs.fyi\\nhttps://www.trueup.io/layoffs\\n\\n\\n\\n\\n\\nWritten By\\nStephanie Kirmer\\nSee all from Stephanie Kirmer\\n\\n\\n\\n\\nArtificial Intelligence, Careers, Editors Pick, Programming, Software Engineering\\n\\nShare This Article\\n\\n\\n \\n\\t\\t\\tShare on Facebook\\t\\t\\n\\n\\n\\n\\n \\n\\t\\t\\tShare on LinkedIn\\t\\t\\n\\n\\n\\n\\n \\n\\t\\t\\tShare on X\\t\\t\\n\\n\\n\\n\\n\\n\\n\\n\\nTowards Data Science is a community publication. Submit your insights to reach our global audience and earn through the TDS Author Payment Program.\\n\\nWrite for TDS\\n\\n\\n\\n\\n\\n\\n\\nRelated Articles\\n\\n\\n\\nWhat Do Large Language Models “Understand”?\\n\\n\\t\\t\\t\\t\\tArtificial Intelligence\\t\\t\\t\\t\\n\\nA deep dive on the meaning of understanding and how it applies to LLMs \\n\\nTarik Dzekman\\nAugust 21, 2024\\n\\n\\t31 min read\\n\\n\\n\\n\\n\\n\\n\\n\\nMaking Sense of the Promise (and Risks) of Large Language Models\\n\\n\\t\\t\\t\\t\\tArtificial Intelligence\\t\\t\\t\\t\\n\\nOur weekly selection of must-read Editors’ Picks and original features \\n\\nTDS Editors\\nApril 27, 2023\\n\\n\\t4 min read\\n\\n\\n\\n\\n\\n\\n\\n\\nDeep Dive into Sora’s Diffusion Transformer (DiT) by Hand ✍︎\\n\\n\\t\\t\\t\\t\\tData Science\\t\\t\\t\\t\\n\\nExplore the secret behind Sora’s state-of-the-art videos \\n\\nSrijanie Dey, PhD\\nApril 2, 2024\\n\\n\\t13 min read\\n\\n\\n\\n\\n\\n\\n\\n\\nBeware of Unreliable Data in Model Evaluation: A LLM Prompt Selection case study with Flan-T5\\n\\n\\t\\t\\t\\t\\tArtificial Intelligence\\t\\t\\t\\t\\n\\nYou may choose suboptimal prompts for your LLM (or make other suboptimal choices via model… \\n\\nChris Mauck\\nJune 16, 2023\\n\\n\\t12 min read\\n\\n\\n\\n\\n\\n\\n\\n\\nBeating ChatGPT 4 in Chess with a Hybrid AI model\\n\\n\\t\\t\\t\\t\\tArtificial Intelligence\\t\\t\\t\\t\\n\\nBetter but not the best, GPT4 cheated but lost the match\\xa0! \\n\\nOctavio Santiago\\nJanuary 22, 2024\\n\\n\\t7 min read\\n\\n\\n\\n\\n\\n\\n\\n\\nSemantic Search Engine for Emojis in 50+ Languages Using AI 😊🌍🚀\\n\\n\\t\\t\\t\\t\\tArtificial Intelligence\\t\\t\\t\\t\\n\\nIf you are on social media like Twitter or LinkedIn, you have probably noticed that… \\n\\nBadr Alabsi, PhD\\nJuly 17, 2024\\n\\n\\t14 min read\\n\\n\\n\\n\\n\\n\\n\\n\\nDeep Dive into Anthropic’s Sparse Autoencoders by Hand ✍️\\n\\n\\t\\t\\t\\t\\tLarge Language Models\\t\\t\\t\\t\\n\\nExplore the concepts behind the interpretability quest for LLMs \\n\\nSrijanie Dey, PhD\\nMay 31, 2024\\n\\n\\t12 min read\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nYouTube\\nX\\nLinkedIn\\nThreads\\nBluesky\\n\\n\\n\\nYour home for data science and Al. The world’s leading publication for data science, data analytics, data engineering, machine learning, and artificial intelligence professionals.\\n\\n\\n\\n\\t\\t©\\n\\t\\nInsight Media Group, LLC\\n\\n\\t\\t2025\\t\\n\\n\\n\\n\\n\\nSubscribe to Our Newsletter\\n\\nWrite For TDSAboutAdvertisePrivacy PolicyTerms of Use\\nCookies Settings\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "url = \"https://towardsdatascience.com/software-engineering-in-the-llm-era/\"\n",
    "loader = WebBaseLoader(url)\n",
    "documents = loader.load()\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d597ae",
   "metadata": {},
   "source": [
    "### Step 2: Chunking the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d651da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "docs = splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6689ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1666d3",
   "metadata": {},
   "source": [
    "### Step 3: Embedding the chunked docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17753811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "vector = embeddings.embed_query(\"France is a country in Europe\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ad45ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3072"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d9bfdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='b99b1cb9-d89d-498d-b37b-68817afb9585', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='Send feedback\\n  \\n  \\n\\n\\n\\nExcept as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License. For details, see the Google Developers Site Policies. Java is a registered trademark of Oracle and/or its affiliates.\\nLast updated 2025-07-18 UTC.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Terms\\n        \\n\\n\\n\\n          Privacy\\n        \\n\\n\\n\\n          Manage cookies\\n        \\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nShqip\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어'),\n",
       " Document(id='3a25235c-8382-4961-a85d-d270b7419f17', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='Community\\n  \\n  \\n\\n\\n\\n\\n\\n\\n\\n                      Google AI Forum\\n                    \\n\\n\\n\\n\\n\\n                      Gemini for Research\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n/\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\nDeutsch\\n\\n\\nEspañol – América Latina\\n\\n\\nFrançais\\n\\n\\nIndonesia\\n\\n\\nItaliano\\n\\n\\nPolski\\n\\n\\nPortuguês – Brasil\\n\\n\\nShqip\\n\\n\\nTiếng Việt\\n\\n\\nTürkçe\\n\\n\\nРусский\\n\\n\\nעברית\\n\\n\\nالعربيّة\\n\\n\\nفارسی\\n\\n\\nहिंदी\\n\\n\\nবাংলা\\n\\n\\nภาษาไทย\\n\\n\\n中文 – 简体\\n\\n\\n中文 – 繁體\\n\\n\\n日本語\\n\\n\\n한국어\\n\\n\\n\\n\\nSign in\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Gemini API docs\\n  \\n    \\n\\n\\n\\n    API Reference\\n  \\n    \\n\\n\\n\\n    Cookbook\\n  \\n    \\n\\n\\n\\n    Community\\n  \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Models\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Gemini API docs\\n   \\n\\n\\n\\n\\n\\n\\n\\n      API Reference\\n   \\n\\n\\n\\n\\n\\n\\n\\n      Cookbook\\n   \\n\\n\\n\\n\\n\\n      Community\\n   \\n\\n\\n\\n\\n\\n\\n\\n      Solutions\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Code assistance\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Showcase\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Community\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started'),\n",
       " Document(id='9894a252-fee5-4805-a9af-86da161fbf7e', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='Gemini for Research\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Veo 3 is now available in the Gemini API! Learn more\\n\\n\\n\\n\\n\\n\\n\\n    \\n        Home\\n      \\n  \\n\\n\\n\\n\\n    \\n        Gemini API\\n      \\n  \\n\\n\\n\\n\\n    \\n        Models\\n      \\n  \\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\\n  \\n  \\n\\n\\n      Embeddings'),\n",
       " Document(id='bff8626e-61bb-49ec-9496-8ba39ca3d02c', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Code assistance\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Showcase\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Community\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\nOverview\\nQuickstart\\nAPI keys\\nLibraries\\nOpenAI compatibility\\n\\nModels\\n\\nAll models\\nPricing\\nRate limits\\nBilling info\\n\\nModel Capabilities\\n\\nText generation\\nImage generation\\nVideo generation\\nSpeech generation\\nMusic generation\\nLong context\\nStructured output\\nThinking\\nFunction calling\\nDocument understanding\\nImage understanding\\nVideo understanding\\nAudio understanding\\n\\nTools\\n\\nGoogle Search\\nCode execution\\nURL context\\n\\nLive API\\n\\nGet started\\nCapabilities\\nTool use\\nSession management\\nEphemeral tokens\\n\\nGuides\\n\\nPrompt engineering\\nContext caching\\nFiles API\\nBatch mode\\nToken counting\\nEmbeddings\\n\\n\\nSafety\\nSafety settingsSafety guidance\\n\\n\\nOpen-Source Frameworks\\nLangChain & LangGraphCrewAILlamaIndex\\n\\nResources\\n\\nMigrate to Gen AI SDK\\nRelease notes\\nAPI troubleshooting\\nFine-tuning')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Embed the chunked documents\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dad2c8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='61b11111-8084-4739-a7e2-2db5bb1b07e4', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='RETRIEVAL_DOCUMENT\\nEmbeddings optimized for document search.\\nIndexing articles, books, or web pages for search.\\n\\n\\nRETRIEVAL_QUERY\\n\\n        Embeddings optimized for general search queries.\\n        Use RETRIEVAL_QUERY for queries; RETRIEVAL_DOCUMENT for documents to be retrieved.\\n    \\nCustom search\\n\\n\\nCODE_RETRIEVAL_QUERY\\n\\n    Embeddings optimized for retrieval of code blocks based on natural language queries.\\n    Use CODE_RETRIEVAL_QUERY for queries; RETRIEVAL_DOCUMENT for code blocks to be retrieved.\\n   \\nCode suggestions and search\\n\\n\\nQUESTION_ANSWERING\\n\\n    Embeddings for questions in a question-answering system, optimized for finding documents that answer the question.\\n    Use QUESTION_ANSWERING for questions; RETRIEVAL_DOCUMENT for documents to be retrieved.\\n   \\nChatbox\\n\\n\\nFACT_VERIFICATION'), Document(id='bff8626e-61bb-49ec-9496-8ba39ca3d02c', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Code assistance\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Showcase\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Community\\n   \\n\\n\\n\\n\\n\\n      More\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGet started\\n\\nOverview\\nQuickstart\\nAPI keys\\nLibraries\\nOpenAI compatibility\\n\\nModels\\n\\nAll models\\nPricing\\nRate limits\\nBilling info\\n\\nModel Capabilities\\n\\nText generation\\nImage generation\\nVideo generation\\nSpeech generation\\nMusic generation\\nLong context\\nStructured output\\nThinking\\nFunction calling\\nDocument understanding\\nImage understanding\\nVideo understanding\\nAudio understanding\\n\\nTools\\n\\nGoogle Search\\nCode execution\\nURL context\\n\\nLive API\\n\\nGet started\\nCapabilities\\nTool use\\nSession management\\nEphemeral tokens\\n\\nGuides\\n\\nPrompt engineering\\nContext caching\\nFiles API\\nBatch mode\\nToken counting\\nEmbeddings\\n\\n\\nSafety\\nSafety settingsSafety guidance\\n\\n\\nOpen-Source Frameworks\\nLangChain & LangGraphCrewAILlamaIndex\\n\\nResources\\n\\nMigrate to Gen AI SDK\\nRelease notes\\nAPI troubleshooting\\nFine-tuning'), Document(id='bfd14e06-7099-4f81-85ee-a792797632e8', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers\\n\\n\\n\\n\\n\\n\\n\\n      \\n      Skip to main content\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Models\\n  \\n    \\n\\n\\n\\n\\n\\nGemini\\n\\n\\n\\n                      About\\n                    \\n\\n\\n\\n\\n\\n                      Docs\\n                    \\n\\n\\n\\n\\n\\n                      API reference\\n                    \\n\\n\\n\\n\\n\\n                      Pricing\\n                    \\n\\n\\n\\n\\n\\n\\nImagen\\n\\n\\n\\n                      About\\n                    \\n\\n\\n\\n\\n\\n                      Docs\\n                    \\n\\n\\n\\n\\n\\n                      Pricing\\n                    \\n\\n\\n\\n\\n\\n\\nVeo\\n\\n\\n\\n                      About\\n                    \\n\\n\\n\\n\\n\\n                      Docs\\n                    \\n\\n\\n\\n\\n\\n                      Pricing\\n                    \\n\\n\\n\\n\\n\\n\\nGemma\\n\\n\\n\\n                      About\\n                    \\n\\n\\n\\n\\n\\n                      Docs\\n                    \\n\\n\\n\\n\\n\\n                      Gemmaverse\\n                    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    Solutions\\n  \\n  \\n\\n\\n\\n\\nBuild with Gemini'), Document(id='60fe5001-5247-4ba5-aa5e-1a174659495c', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='language models, guiding it to generate more informed and accurate responses.\\nFor enterprise-grade applications and high-volume workloads, we suggest using\\nembedding models on\\nVertex AI.\\nGenerating embeddings\\nUse the embedContent method to generate text embeddings:')]\n"
     ]
    }
   ],
   "source": [
    "answer = vectorstore.similarity_search(\"Unlike Generative AI model that\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b712790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer the question based on the context below.\\n\\n\\n    <context>\\n    {context}\\n\\n\\n    </context>\\n\\n    '), additional_kwargs={})])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.1, max_output_tokens=1024, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001F523998460>, default_metadata=(), model_kwargs={})\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"Answer the question based on the context below.\\n\\n\n",
    "    <context>\n",
    "    {context}\\n\\n\n",
    "    </context>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "document_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    ")\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90a101bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F5723A19C0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer the question based on the context below.\\n\\n\\n    <context>\\n    {context}\\n\\n\\n    </context>\\n\\n    '), additional_kwargs={})])\n",
       "            | ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.1, max_output_tokens=1024, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000001F523998460>, default_metadata=(), model_kwargs={})\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriever = vectorstore.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    retriever,document_chain\n",
    ")\n",
    "retrieval_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "361c3a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text, Gemini Embedding models are used to transform input data (words, phrases, sentences, and code) into numerical representations called embeddings.  These embeddings are then used in various Natural Language Processing (NLP) tasks such as semantic search, classification, and clustering, improving accuracy and context awareness compared to keyword-based methods.  A key application is building Retrieval Augmented Generation (RAG) systems, where embeddings help retrieve relevant information from knowledge bases to enhance the accuracy and context of language model outputs.  For large-scale applications, using Vertex AI is recommended.  The text also mentions storing embeddings in vector databases like BigQuery, AlloyDB, Cloud SQL, ChromaDB, Qdrant, Weaviate, and Pinecone.\n"
     ]
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\n",
    "    \"input\": \"Using embeddings in Gemini\",\n",
    "    \n",
    "})\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63620e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Using embeddings in Gemini', 'context': [Document(id='9894a252-fee5-4805-a9af-86da161fbf7e', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='Gemini for Research\\n   \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n          Veo 3 is now available in the Gemini API! Learn more\\n\\n\\n\\n\\n\\n\\n\\n    \\n        Home\\n      \\n  \\n\\n\\n\\n\\n    \\n        Gemini API\\n      \\n  \\n\\n\\n\\n\\n    \\n        Models\\n      \\n  \\n\\n\\n\\n\\n\\n\\n\\n  \\n    \\n    Send feedback\\n  \\n  \\n\\n\\n      Embeddings'), Document(id='133bb7e0-84b4-4d75-9301-8a727725f81d', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content=\"embedding-001 (Deprecating on August 14, 2025)\\ntext-embedding-004 (Deprecating on January 14, 2026)\\n\\nUsing embeddings\\nUnlike generative AI models that create new content, the Gemini Embedding model\\nis only intended to transform the format of your input data into a numerical\\nrepresentation. While Google is responsible for providing an embedding model\\nthat transforms the format of your input data to the numerical-format requested,\\nusers retain full responsibility for the data they input and the resulting\\nembeddings. By using the Gemini Embedding model you confirm that you have the\\nnecessary rights to any content that you upload. Do not generate content that\\ninfringes on others’ intellectual property or privacy rights. Your use of this\\nservice is subject to our Prohibited Use\\nPolicy and\\nGoogle's Terms of Service.\\nStart building with embeddings\\nCheck out the\\nembeddings quickstart notebook\\nto explore the model capabilities and learn how to customize and visualize your\\nembeddings.\"), Document(id='9fec3598-959a-4b69-bc20-d5b0581913db', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='Note: gemini-embedding-001 is our newest text embedding model available in the\\n      Gemini API and Vertex AI.\\nThe Gemini API offers text embedding models to generate embeddings for words,\\nphrases, sentences, and code. These foundational embeddings power advanced NLP\\ntasks such as semantic search, classification, and clustering, providing more\\naccurate, context-aware results than keyword-based approaches.\\nBuilding Retrieval Augmented Generation (RAG) systems is a common use case for\\nembeddings. Embeddings play a key role in significantly enhancing model outputs\\nwith improved factual accuracy, coherence, and contextual richness. They\\nefficiently retrieve relevant information from knowledge bases, represented by\\nembeddings, which are then passed as additional context in the input prompt to\\nlanguage models, guiding it to generate more informed and accurate responses.\\nFor enterprise-grade applications and high-volume workloads, we suggest using\\nembedding models on\\nVertex AI.'), Document(id='584b91b2-1999-47b1-bb90-31e9079549da', metadata={'source': 'https://ai.google.dev/gemini-api/docs/embeddings', 'title': 'Embeddings \\xa0|\\xa0 Gemini API \\xa0|\\xa0 Google AI for Developers', 'language': 'en'}, page_content='Document search tutorialtask\\n\\nAnomaly detection: Comparing groups of embeddings can help identify hidden\\ntrends or outliers.\\n\\n    Anomaly detection tutorialbubble_chart\\n\\nClassification: Automatically categorize text based on its content, such\\nas sentiment analysis or spam detection\\n\\n    Classification tutorialtoken\\n\\nClustering: An effective way to grasp relationships is to create clusters and visualizations of your embeddings.\\n\\n    Clustering visualization tutorialbubble_chart\\n\\n\\nStoring embeddings\\nAs you take embeddings to production, it is common to\\nuse vector databases to efficiently store, index, and retrieve\\nhigh-dimensional embeddings. Google Cloud offers managed data services that\\ncan be used for this purpose including\\nBigQuery,\\nAlloyDB, and\\nCloud SQL.\\nThe following tutorials show how to use other third party vector databases\\nwith Gemini Embedding.\\n\\n\\n    ChromaDB tutorialsbolt\\n\\n\\n    QDrant tutorialsbolt\\n\\n\\n    Weaviate tutorialsbolt\\n\\n\\n    Pinecone tutorialsbolt')], 'answer': 'Based on the provided text, Gemini Embedding models are used to transform input data (words, phrases, sentences, and code) into numerical representations called embeddings.  These embeddings are then used in various Natural Language Processing (NLP) tasks such as semantic search, classification, and clustering, improving accuracy and context awareness compared to keyword-based methods.  A key application is building Retrieval Augmented Generation (RAG) systems, where embeddings help retrieve relevant information from knowledge bases to enhance the accuracy and context of language model outputs.  For large-scale applications, using Vertex AI is recommended.  The text also mentions storing embeddings in vector databases like BigQuery, AlloyDB, Cloud SQL, ChromaDB, Qdrant, Weaviate, and Pinecone.'}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63e37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
